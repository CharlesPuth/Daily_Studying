```python
"""
@author: ChangLiang
@contact: 1452380548cl@gmail.com
@time: 2021/2/2 0008 20:00
"""
```



## 1.FCN详解（网络架构）

### 1.1前馈网络：VGG-16

特征层的提取用的一般是VGG或者其他的图像识别网络，作用是提取出不同大小的特征层，供给upsample上采样层使用；作者在这里也对比了使用不同的前馈神经网络的效果，可以看到VGG是效果比较好的。文章也选用VGG16作为特征提取的前馈网络。

### 1.2 网络架构图解：对比观察

FCN框架：FCN是输入和输出都是图像，没有全连接层。较浅的高分辨率层用来解决像素定位的问题，较深的层用来解决像素分类的问题。属于端到端的学习，图像风格转换以及图像超分辨率都是这类框架。

**FCN中常见的s代表的是stride的大小，从这个角度来介绍,注意对比**

#### 1.基础架构

以对Pascal VOC数据集进行语义分割为例，则基于AlexNet特征提取的FCN网络的结构为（这是8s的图）：

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210203174127862.png" alt="image-20210203174127862" style="zoom: 67%;" />

其中，输入图像大小为500x500x3，预测输出维度为500x500x21(20类+背景)，蓝色竖线表示卷积运算，绿色竖线表示池化运算，灰色竖线表示crop运算；

#### 2.FCN-32s架构

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210203205521495.png" alt="image-20210203205521495"  />

输入图像大小为500x500x3，卷积网络阶段获得的特征图为16x16x21；然后反卷积运算的步长为32直接获得最终的预测结果；

#### 3.FCN-16s架构

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210203205730452.png" alt="image-20210203205730452" style="zoom:80%;" />

输入图像大小为500*500*3，卷积网络阶段获得的特征图为16x16x21；然后第一次反卷积运算的步长为2获得34x34x21的特征图，并将其与pool4的特征图进行连接；最后进行第二次反卷积运算，步长为16获得最终的预测结果；

#### 4.FCN-8s架构（Main）

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210203205811112.png" alt="image-20210203205811112" style="zoom:80%;" />

输入图像大小为500x500x3，卷积网络阶段获得的特征图为16x16x21；然后第一次反卷积运算的步长为2获得34x34x21的特征图，并将其与pool4的特征图进行连接；接着进行第二次反卷积运算，步长为2获得70x70x21的特征图，并将其与pool3的特征图进行连接；最后进行第三次反卷积运算，步长为8,获得最终的预测结果；

### 1.3 网络架构数据分析：如何进行特征融合

以w,h表示图像和宽高，整个FCN网络基本原理如下所示**（只是原理示意图）**：

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210203210920952.png" alt="image-20210203210920952" style="zoom:80%;" />

#### **1.在下采样阶段：**

1. image经过多个conv和+一个max pooling变为pool1 feature，宽高变为1/2
2. pool1 feature再经过多个conv+一个max pooling变为pool2 feature，宽高变为1/4
3. pool2 feature再经过多个conv+一个max pooling变为pool3 feature，宽高变为1/8
4. ……（重复步骤）
5. 直到pool5 feature，宽高变为1/32。

#### **2.在上采样阶段（注：最后都有softmax进行分类预测）：**

1.对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即分割图）。

2/对于FCN-16s，首先对pool5 feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature**逐点相加**，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。

3.对于FCN-8s，首先进行pool4+2x upsampled feature**逐点相加**，然后又进行pool3+2x upsampled**逐点相加**，即进行更多次特征融合。具体过程与16s类似，不再赘述。

**（注：1.这里的倍数，就比如2倍，就是在特征图每个特征点之间再填充一行（或一列），然后再进行卷积操作；2.从底层网上层是一个迭代相加的过程，8s中包含了16s和32s计算的结果）**

#### **3.预测阶段：**

FCN的全卷积与上采样，在上采样最后加上softmax，就可以对不同类别的大小概率进行估计，实现端到端（end to end）的学习，最后输出的图是一个概率估计，对应像素点的值越大，其像素为该类的结果也越大。

### Tips-

**（逐点相加需要注意一下，因为Unet与FCN的基本不同就包括了把逐点相加进行了改进）**