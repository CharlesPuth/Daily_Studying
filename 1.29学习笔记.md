```python
"""
@author: ChangLiang
@contact: 1452380548cl@gmail.com
@time: 2021/1/29 0008 20:00
"""
```



## 1.Pythorch神经网络工具箱：nn

### 1.1 Module的概念

'Module'是一个抽象的概念，torch.nn的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承`nn.Module`，撰写自己的网络/层。

### 1.2 网络的定义

只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。所以我们需要自行定义forward

在PyTorch中，model由一个常规的Python类表示，该类继承自Module类。

它需要实现的最基本的方法是:

`__init__(self)`定义了组成模型的两个参数:a和b。

> 模型可以包含其他模型作为它的属性，所以可以很容易实现嵌套。

`forward(self, x)`:它执行了实际的计算，也就是说，给定输入x，它输出一个预测。

若为回归任务构建一个适当的(但简单的)模型:

```python
class ManualLinearRegression(nn.Module):
    def __init__(self):
        super().__init__()
        # To make "a" and "b" real parameters of the model, we need to wrap them with nn.Parameter
        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))
        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))
        
    def forward(self, x):
        # Computes the outputs / predictions
        return self.a + self.b * x
```

在定义中，需要注意的点有：

```
(1) 自定义层（上面的ManualLinearRegression）必须继承nn.Module，  
注：super(Linear, self).__init__()等价于nn.Module.__init__(self)

(2)在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter, Parameter是一种特殊的Variable, 其默认是自动求导的

(3)forward函数实现前向传播，

(4)无需写反向传播函数，因为前向传播都是对variable进行操作。（而且forward定义好了backward就可以直接得到了）

(5)Module中学习的参数可以通过named_parameters()或者parameters()返回迭代器（上述代码不涉及）
```

### 1.3 前馈网络相关知识（nn.Sequential）/ModuleList

定义：一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，同时以神经网络模块为元素的有序字典也可以作为传入参数。通俗说法是在定义过程中一层一层放进去各种网络层最终形成整体网络架构。

有如下三种方法来定义：

```python
# 方式1
net1 = nn.Sequential()
net1.add_module('conv', nn.Conv2d(3, 3, 3))
net1.add_module('batchnorm', nn.BatchNorm2d(3))
net1.add_module('activation_layer', nn.ReLU())

# 方式2
net2 = nn.Sequential(
    nn.Conv2d(3, 3, 3),
    nn.BatchNorm2d(3),
    nn.ReLU()
)

# 方式3
from collections import OrderedDict
net3 = nn.Sequential(OrderedDict([
    ('conv', nn.Conv2d(3, 3, 3)),
    ('bn1', nn.BatchNorm2d(3)),
    ('relu1', nn.ReLU())
]))
```

ModuleList是另一种等价的概念，定义如下：

```python
nn.ModuleList([nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 2)])
```

### 1.4 优化器

具体使用的优化器目前有很多，如Adam等等；导入之后选择合适的优化器去使用：

```python
from torch import optim
```

### 1.5 一个简单的建立三层神经网络的例子

通过Squential将网络层和激活函数结合起来，输出激活后的网络节点。

```python
#hyper parameters
in_dim=1
n_hidden_1=1
n_hidden_2=1
out_dim=1

class Net(nn.Module):
    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super().__init__()
        self.layer = nn.Sequential(
              nn.Linear(in_dim, n_hidden_1),  #容器内放入的第一层
              nn.ReLU(True)，#容器内放入的激活函数
              nn.Linear(n_hidden_1, n_hidden_2)，#容器内放入的第二层
              nn.ReLU(True)，#容器内放入的第二层后的激活函数
              # 最后一层不需要添加激活函数
              nn.Linear(n_hidden_2, out_dim)
                 )

    def forward(self, x):
        x = self.layer(x)
        return x
```



## 2 强化学习中DQN的使用

### 2.1 DQN的一些需要特别注意的原理

DQN实际上由两个结构相同但参数更新速度不同的网络构成，也即先创建一个Net类来将网络架构和forward函数定义出来（即上文中的内容），再利用Net创建两个神经网络: 评估网络（valuate）和目标网络（target），评估网络中是不断每一步更新参数的，目标网络经过若干步才会更新一次，这是一种避免陷入局部最优的选择。目前还有一些没有太清楚的东西，涉及一些Q-learning的内部知识，比如Q估计和Q实际等等。

### 2.2  从DQN到其他强化学习算法

算法内部应该都包含如下几个部分来完成：

```python
def __init__(self):  #用来定义模型（DQN这里就是网络）

def choose_action(self, x):  #做出决策的函数，也作为调用网络（训练）时候进行参数饭回的函数

def store_transition(self, s, a, r, s_):  #储存历代结果（也即记忆库），用于off-policy离线学习

def learn(self):  #定义学习函数(记忆库已满后便开始学习)
```

调用方面都由以下几个部分来完成（代码比较实体化，不太好概括，大体的逻辑如下）：

```text
初始化：定义step以及reset等 -> 得到初始观测值 ->

传入初始观测值得到action -> 由action得到reward和下一状态观测值      （循环进行）

积累到一定的记忆库就进行一次学习
```

