```python
"""
@author: ChangLiang
@contact: 1452380548cl@gmail.com
@time: 2021/1/28 0008 20:00
"""
```



### 1. Python基本语法复习

####  1.1 关于类的理解（class）

“类”概念的加深理解：相当于C语言中的结构体，自己构建的数据类型。

对一个变量进行类的赋值之后（严格意义上的实例化），这个变量就变成了所定义的class的数据类型，而后可以在class的定义中所定义的变量（**实例变量**）继续用传入的参数对变量进行实例定义（self. variable），class中定义的方法也可以用实例变量进行定义和调用。继续调用类中所定义的方法采用

```python
variable.method()
```

来进行。实例化和调用时需要传入的参数以实际情况为准。self位不需要参数。

#### 1.2 Numpy 的广播机制的作用

用于不同形状的数组之间能够相互运算（在线性代数中是不能直接进行的）

#### 1.3 Jupyter notebook

利用终端打开或者进入127.0.0.1:8888

功能：Ctrl+enter可以像ipython一样进行部分代码运行，可以进行代码测试

### 2. Torch基本概念学习

本章主要介绍了PyTorch中两个基础底层的数据结构：Tensor和autograd中的Variable。Tensor是一个类似Numpy数组的高效多维数值运算数据结构，有着和Numpy相类似的接口，并提供简单易用的GPU加速。Variable是autograd封装了Tensor并提供自动求导技术的，具有和Tensor几乎一样的接口。`autograd`是PyTorch的自动微分引擎，采用动态计算图技术，能够快速高效的计算导数。

#### 2.1  tensor（张量）与autograd（自动求导-计算图）

pytorch中的tensor与numpy的arrays可以便捷地进行相互转换，本质都是一维向量、二维或多维数组。不同（优点）在于所有框架下的tensor可以进行GPU计算加速。

##### 2.1.1 基础操作

从接口的角度而言，对tensor的操作可以分为两类：

```python
torch.function #将被操作的tensor都放在function中

tensor.function #tensor即为被操作的张量，若同时有另一张量被操作，则放在function中
```

从存储的角度而言，对tensor的操作可以分为两类：

(1)不修改自身数据，饭回新的tensor

(2)修改自身数据，运算结果存在被操作的tensor下

当然无论如何，方法都是可以连用的，就比如

```python
a = t.arange(0,16).view(4.4)
```

下面是一些实用操作：

###### 1.tensor与array互相转换

```python
x = t.Tensor(5,3) #二维向量组
b = x.numpy() #将tensor也即x转化为numpy中的array即b（tensor->numpy）
x = b.from_numpy(a) #将array即a转换为tensor即x（numpy->tensor）
```

###### 2.创建tensor

```python
a = t.Tensor(2,3) #2行3列
```

###### 3.修改维度

```python
b = a.view(2,3)  #将a修改到2行3列
```

###### 4.索引

同numpy中的array

```python
a[0] #第0行

a[:,0] #第0列

a[0][2] #第0行第2个元素（等价a[0,2]）

a[0,-1] #第0行最后一个元素
```

###### 5.tensor类型

可以通过指令转换tensor类型（具体分析）

###### 6.逐个元素操作

通常是运算、激活、截断等操作。如

```python
a = t.arange(0,6).view(2,3)

t.cos(a) #这样就是对每一个元素进行cos运算
t.tanh(a) #自己举得激活例子，不一定对
```

###### 7.归并操作

如累加、求中位数平均数等；将一个tensor，沿着某一维度进行指定操作使输出形状小于输入形状（例子略）

###### 8.比较操作

比较、取得最大最小值、排序等等，有的会饭回bool值（例子略）

#### 2.2 Autograd

提出意义：自动计算反向传播时候梯度的概念。实例就是在线性回归算法计算loss后的梯度反向传播中，需要手动计算梯度更新参数，如

```python
# backward：手动计算梯度
dloss = 1
dy_pred = dloss * (y_pred - y)
dw = x.t().mm(dy_pred)
db = dy_pred.sum()
# 更新参数
w.sub_(lr * dw)
b.sub_(lr * db)
```

手动计算梯度就是反向传播的过程，在传统情况下需要自己编写公式代码，autograd模块完成了这项功能但不止于此，整个模块的功能如下所示：

###### 1.Variable-数据结构

是autograd的核心数据结构，由三部分组成：

- `data`：保存variable所包含的tensor（值得注意的是，只有对variable的操作才能使用autograd，如果对variable的data直接进行操作，将无法使用反向传播。除了对参数初始化，一般我们不会修改variable.data的值。）
- `grad`：保存`data`对应的梯度，`grad`也是variable，而不是tensor，它与`data`形状一致。 
- `grad_fn`： 指向一个`Function`，记录tensor的操作历史，即它是什么操作的输出，用来构建计算图。如果某一个变量是由用户创建，则它为叶子节点，对应的grad_fn等于None。

###### 2.Variable基本操作

```python
x.grad #最简单的计算梯度（倒数）的公式
```

Variable的构造函数需要传入tensor，同时有两个可选参数：

- `requires_grad (bool)`：是否需要对该variable进行求导
- `volatile (bool)`：意为”挥发“，设置为True，则构建在该variable之上的图都不会求导，专为推理阶段设计

```python
a = V(t.ones(3,4), requires_grad = True) 
```

如果想要计算各个Variable的梯度，只需调用根节点variable的`backward`方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。

`variable.backward(grad_variables=None, retain_graph=None, create_graph=None)`主要有如下参数：

- grad_variables：形状与variable一致，对于`y.backward()`，grad_variables相当于链式法则中的。grad_variables也可以是tensor或序列。
- retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。
- create_graph：对反向传播过程再次构建计算图，可通过`backward of backward`实现求高阶导数。

3.实例（对应tensor）

下面用一个实例说明autograd在反向传播过程中有多便捷，与前文的手动反向传播代码进行对比

```python
# backward：手动计算梯度
loss.backward()
# 更新参数
w.data.sub_(lr * w.grad.data)
b.data.sub_(lr * b.grad.data)
```



#### 2.3 计算图

概念理解：有向无环图，叶子节点为输入根节点为输出，所以反向传播是从根节点向叶子节点进行链式求导。图中存储两类对象：变量（常用椭圆表示）、算子（也即进行的函数、方法）；

在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：

- 使用autograd.grad函数
- 使用hook

`autograd.grad`和`hook`方法都是很强大的工具，更详细的用法参考官方api文档。

用autograd实现的线性回归最大的不同点就在于autograd不需要计算反向传播，可以自动计算微分。

