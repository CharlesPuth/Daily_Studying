```python
"""
@author: ChangLiang
@contact: 1452380548cl@gmail.com
@time: 2021/1/31 0008 20:00
"""
```



## 1.CNN重点提要

### 1.1 从全连接深度神经网络到CNN

为什么DNN（深度神经网络）在训练后能够正确地分类？那肯定是它学到了东西，学到什么东西呢？它学到了图片中的某些空间结构，不同数字它们的空间结构肯定是不一样的，而这样的空间结构就是**由像素点与像素点之间的关系形成**。我们再仔细看DNN输入层和第一个隐藏层，发现它对我们输入的784个像素点是同等对待的，也就是说它此时并没有考虑像素点与像素点之间的关系。有没有一个好点的模型能够考虑到这点呢？那就是CNN：考虑像素点之间关系的神经网络；换言之，像素点之间互相有联系的神经网络。

### 1.2 CNN的三大核心思想：局部感受野(local receptive fields) 权值共享(shared weights) 池化(pooling)

#### 1.2.1局部感受野(local receptive fields) 

**用“窗口”在输入图像中滑动的思想。**

DNN中，我们会把输入层的每个神经元都与第一个隐藏层的每个神经元连接。而在CNN中我们这样做的，第一个隐藏层的神经元只与**局部**（卷积核大小）区域输入层的神经元相连。这里的局部区域就是**局部感受野**，它像一个架在输入层上的窗口。一个隐藏层的神经元会学习分析它”视野范围“(局部感受野)里的特征。

##### Tips-

1.图片大小（x）与窗口大小（a）与卷积后隐藏层（b）的计算关系： **b = x-a+1**；2.存在的超参数：stride-窗口一次划动几个像素点；窗口大小：卷积核大小。

#### 1.2.2 权值共享(shared weights) 

**每一种滑动窗口的参数（权值）无论滑倒何处都相同**

在权值共享下，窗口移来移去还是同一个窗口，也就意味着第一个隐藏层所有的神经元从输入层探测到的是同一种特征，只是从输入层的不同位置探测到(图片的中间，左上角，右下角等等)。必须强调的是，**一个窗口只能学到一种特征**！另外，窗口还有其他叫法：卷积核(kernal),滤波器(filter)。在做图像识别学习中一个特征肯定是不够的，我们想要学习更多的特征，就需要更多的窗口。窗口与窗口间的运算矩阵w和偏置b是不共享的，三个窗口就表示有三个w矩阵和三个偏移值b，结果是从整张图片的各个位置学到三种不同的特征。公式如下（其中括号外是激活，*表示卷积）：

![image-20210131183340204](C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210131183340204.png)

**模型学习的参数就是我们的卷积核以及偏置。每1个5x5的卷积核就是25个参数，再加上一个偏置，共26的参数来供学习。**

##### Tips-

权值共享还有一个很大的好处，就是可以大大**减少模型参数**的个数。一个窗口参数个数是26(5x5+1),20个窗口就是520个参数，如果换成全连接的话就是785(28*28+1)个参数，比CNN多了265个参数。

#### 1.2.3池化(Pooling)

CNN还有一个重要思想就是池化，池化层通常接在卷积层后面。引入它的目的就是为了**简化卷积层的输出**。通俗地理解，池化层也在卷积层上架了一个窗口，但这个窗口比卷积层的窗口简单许多，不需要w，b这些参数，它只是对窗口范围内的神经元做简单的操作，如求和，求最大值这种在窗口范围内只留下一个值或者只计算得到一个值，把求得的值作为池化层神经元的输入值。经过池化后，大大减少了我们学到的特征值，也就大大减少了后面网络层的参数，使得训练起来更快速便捷。通常卷积层的窗口是多个的，池化层的窗口也是多个的。（还是一样，提取不同的特征，用不同的窗口。）简单来说，卷积层用一个或者多个窗口去对输入层做卷积操作，池化层也用一个或者多个窗口去对卷积层做池化操作。（通常为了提取不同的特征，卷积层的窗口是多个的，从而其后面跟着的池化层的窗口也应对应是多个的。）

##### Tips-

1.常用的对一个窗口内**求最大值**的方法，称之为max-pooling；还有一种比较常用的是L2-pooling,与max-pooling唯一的区别就是在池化窗口扫过的区域里做的操作不是求最大值，而是**所有神经元平方后求和再开根号**，这和我们L2正则对权值参数的操作是一样的。实际操作中，这两种方式都是比较常用的。池化**操作方式的选择**也是我们**调参**工作的一部分，我们可以根据validation data集（相当于验证集）来调节，选择更好的池化操作。

2.举例来看起到的简化作用：定义一个一个2x2的窗口，设定此时的窗口每次移动两步，采用的是求最大值的方法，（max-pooling），开始时卷积层含有24x24个神经元，经过池化后到池化层就是12*12个神经元，成功简化。

3.池化层可以保留相对位置信息（A在B左边，池化之后A还是在B左边），这种位置关系的明确也有利于特征处理后图像的恢复。

#### 1.2.4 通常会和卷积层放在一起的简单概念：激活（也称非线性层 Non-linearity Layer）

现在已经不常将激活函数单独算作一层了，而是当作卷积层经过卷积之后做的一个运算（也即卷积层一共有两个工作：卷积+激活）。在非线性层中，一般使用ReLU激活函数，而不是使用传统的Sigmoid或Tan-H激活函数（已淘汰）。

在应用上来说，对于输入图像中的每个负值，ReLU激活函数都返回0值，而对于输入图像中的每个正值，它返回相同的值。（-4返回0，14返回14）.从原理上来说，如果多个卷积层之间不增加激活函数，那么多个卷积层的卷积运算就可以利用结合律提前进行运算，就可以看作多个卷积就只对输入图像进行了一次总的卷积，只存在一层卷积层。增加激活函数相当于把各个层的卷积独立开来，增加了非线性的因素进去。**（后文提到卷积和卷积层也默认是卷积+激活的连环操作）**

#### 1.2.5 从池化到最终的特征输出：全连接层

**全连接层中的神经元与输入数据中的全部区域都连接，并且参数各不相同。可以看作是每一个全连接层的神经元感受野都是全部的输入数据，并且都有各自的w和b，是一种特殊的卷积**

一般对于分10类的任务，我们的输出是一个1x10的矩阵（向量），但是经过所有操作最终得到的池化层往往和它的维度完全不一致，那么如何进行全连接呢？

两张图解释如何进行全连接：

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210131230336726.png" alt="image-20210131230336726" style="zoom:67%;" /><img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210131230530558.png" alt="image-20210131230530558" style="zoom:67%;" />

以上表达的是，前图中实现了每一个Y都有X输入（也即每一个预测结果都受到所有特征的影响），那么通过定义一种运算（如后图的矩阵运算），使得每一个输出神经元都含有输入神经元，受每一个输入神经元的影响，这就是全连接。

##### Tips-

全连接层和卷积层中的神经元都是计算点积和非线性激活，函数形式是一样的，唯一的差别在于卷积层中的神经元只与输入数据中的一个局部区域连接，并且采用参数共享；而全连接层中的神经元与输入数据中的全部区域都连接，并且参数各不相同。因此，两者是可能相互转化的。鉴于此，也有最新的论文用卷积层替换了全连接层，来减少参数，因为全连接层参数实在过多，不利于训练）

#### 1.2.6 小总结

局部感受野就是卷积的时候用一个窗口来提取特征，每个窗口只提取窗口内的特征；权值共享就是同一种窗口在滑动的过程中对无论何处的窗口内的像素点做的运算都是相同的；池化就是用另外一种窗口来简化（变小）卷积后的结果，相当于是对特征进行进一步浓缩。

最简单的CNN从左往右依次是输入层，卷积层，池化层，输出层。输入层到卷积层是卷积操作，卷积层到池化层是池化操作。池化层到输出层是全连接，这和DNN是一样的。一般性的层的组合模式是：先堆叠一个或多个卷积层进行特征提取，然后接一个池化层进行空间尺寸缩小，之后重复此模式，直到空间尺寸足够小（如7×7和5×5），最后接多个全连接层，其中最后一个全连接层输出类别分值。CNN都是一层一层组合起来的，层与层之间的行为也是有对应的权值w和偏移值b决定的，并且它们的目的也是一致的:**通过training data来学习网络结构中的w和b，从而能把输入的图片正确分类。**

### 1.3 CNN中的反向传播（BP）

#### 1.3.1 从DNN的反向传播开始说起——三个重要概念：损失函数、梯度下降、反向传播

**一言以蔽之：用梯度下降使损失函数最小化。**

梯度下降是方法，目的是在反向传播中计算出合适的w和b（上文提到的，权重与偏置）来使得损失函数最小化。下面逐个介绍：

##### 1.损失函数

损失函数就是输入的数据经过一系列运算后的输出与预期输出的差距（预测值与实际值的差距），比如对于m个个体的样本（xi,yi），损失函数就可以定义为

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210131235736174.png" alt="image-20210131235736174" style="zoom:67%;" />

其中hθ(xi)为第i个样本经过计算后得到的结果，yi表示第i个样本的实际对应值。在CNN模型中，卷积层的运算又都是卷积+激活，卷积就是乘以权重再加偏置，所以以上的hθ(xi)函数就可以被清晰定义出来做运算，得到的J(θ0,θ1)就是损失函数。

##### **2.梯度与梯度向量**

梯度是指某一函数在该点处的**方向导数**（在一元函数里就是斜率的意思）沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）**变化最快**，变化率最大（在一元函数中就是导函数取得最大值：也即斜率最大时）。在CNN中，也即在最终的损失函数

##### **3.梯度下降**

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，梯度减少最快，也就是更加容易找到函数的最小值。梯度下降即是沿着梯度向量的反方向来寻找损失函数的最小值的。形象来说，在山顶沿着坡坡最陡的地方下山走的最快。

##### **4.梯度下降如何改变我们的参数（w,b）**

明白了梯度下降的原理方法，那么我们怎么用梯度下降来完成参数的更新？我们只需要计算出影响损失函数的各个因素（即损失函数中包含的自变量）的梯度，然后乘以一个自己定义的**学习率（关键概念）**，再用原来的参数±这个值（加还是减取决于具体情境）就可以得到更新后的参数，如下面公式所示

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210201001252739.png" alt="image-20210201001252739" style="zoom: 67%;" />

其中*θi+1* 为更新后的参数，*θi*为初始参数，α 为学习率，∇J（θi）为损失函数关于*θi*的梯度。

##### 5.反向传播：链式求导

反向传播算法即从输出出发，通过一级一级的向前求导数或者偏导数来求得损失函数中各个变量的梯度值，来从后往前不断更新参数的值。具体原因是因为不是所有的参数值都是直接连接到输出层的，中间隔了别的层就相当于是一个复合函数的存在，需要借助链式求导法则来求多次导数得到对应的梯度。具体的数学推导方法没有细看，代码里已经集成了反向传播的功能，只要定义好前向传播就会自动得到反向传播的具体运算，如pytorch的autograd和集成的nn工具箱。

#### 1.3.2具体到层：不同的层怎么进行反向传播

##### 1.全连接层

全连接层在上面的介绍中也已经知道，一个全连接层的运算通过乘以w权重再加上b的偏置最后利用激活函数激活来完成。在全连接层中，反向传播与任何常规人工神经网络完全相同，在反向传播中（使用梯度下降作为优化算法），使用损失函数的偏导数即损失函数关于权重的导数来更新参数（就是上文方案）。在反向传播中我们将损失函数的导数与激活后输出做偏导计算，激活输出的导数与非激活输出做偏导计算，导数为未激活的输出与权重做偏导计算，再根据链式法则将以上三者乘在一起得到对于权重w5的梯度。如下图

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210201002428244.png" alt="image-20210201002428244" style="zoom:67%;" />

其中net01和out01之间是激活函数操作，Etotal的定义是损失函数，E01是图示神经元的损失函数值，同时假定还有另外一个损失E02存在共同构成总的损失函数。

##### 2.池化层：

Maxpool 池化层反向传播，除最大值处继承上层梯度外，其他位置置零。再向上一级进行传播。如下图

<img src="C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210201003300271.png" alt="image-20210201003300271" style="zoom:50%;" />

不同的池化方式的反向传播方法不同，但无妨，因为定义池化方法的时候就已经定义好了反向传播方法，比如在平均池化层中，梯度是通过所有的输入（在平均合并之前）进行传播，这个不需要操心。

##### 3.卷积层

也是由w和b组成的层，参考全连接层的方案。

##### 4.小总结

前向传播时重点，因为要学会设计网络；个人认为反向传播理解概念就行，因为现在的框架已经集成了反向传播的功能，调包即实现。

##### Tips-

1.梯度下降大家族有批量梯度下降法（BSG，Batch Gradient Descent）随机梯度下降法（SGD，Stochastic Gradient Descent）小批量梯度下降法（MBGD，Mini-batch Gradient Descent）等等多种方案，可以具体选用时候细致了解。

2.学习率设置很重要，学习率被定义为每次迭代中成本函数中最小化的量。也即下降到成本函数的最小值的速率是学习率，它是可变的重要参数，设置不当就会导致陷入局部最优或者其他情况。

## 2.网络基本理论补充-从CNN到FCN

### 2.1 “三剑客”与全连接层：U-net中放弃的全连接层FC究竟是什么

引言：在卷积神经网络尚未火热的年代，人们使用haar/lbp + adaboost级连的组合方式检测人脸，hog+svm的组合方式检测行人。这种传统的目标检测方法一个认知上的优势就是: **模块的功能明确，划分得很清晰，符合人们的理解方式。**其中，haar，lbp，hog等手工设计的特征提取算子用于**提取特征**，adaboost，svm用于**对提取的特征分类**。

而卷积神经网络中**conv+relu(早期为sigmoid)+pooling(以下称三剑客)**的组合，不仅可以替代手工设计特征算子的繁琐，而且局部感受野+权值共享的设计思想也能避免全连接网络中的种种弊端。此时人们将三剑客的组合视为**特征提取**的过程，如果按照早期人们特征提取+分类的设计思路，那么分类使用全连接的设计方式，就可以刚好实现了一个end-to-end的架构，也即早起卷积神经网络的原型。

但必须明白的是，虽然模型完成了一个end-to-end的设计架构，可以直接用于训练和分类，但在人们的认知上，特征提取和分类依然是分开的，也就是说**三剑客用于特征提取，全连接用于分类。**

后来随着更优秀分类网络的出现(alexnet，vgg等)，人们不再仅仅满足于分类准确率的提升，面对动辄两三百M的模型，人们思考能否减少模型的大小。人们通过研究发现，在包含全连接的网络中，全连接的参数占据了所有参数中的大部分比例，这样使得精简全连接参数变得刻不容缓。目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程，也能取得很好的效果。

### 2.2  FC与卷积的异同

究其内里，全连接层从定义而言还是做一个卷积操作。卷积层本来就是全连接的一种简化形式:不全连接+参数共享，同时还保留了空间位置信息。这样大大减少了参数并且使得训练变得可控。
全连接就是个矩阵乘法，相当于一个特征空间变换，可以把有用的信息提取整合。再加上激活函数的非线性映射，多层全连接层理论上可以模拟任何非线性变换。但缺点也很明显: **无法保持空间结构**。全连接的一个作用是**维度变换**，尤其是可以把**高维变到低维**，同时把有用的信息保留下来。全连接另一个作用是隐含语义的表达(embedding)，把原始特征映射到各个隐语义节点(hidden node)。对于最后一层全连接而言，就是分类的显示表达。（不同channel同一位置上的全连接等价与1x1的卷积。）

全连接的核心操作就是矩阵向量乘积

![[公式]](https://www.zhihu.com/equation?tex=y+%3D+Wx)

本质就是由一个特征空间线性变换到另一个特征空间。目标空间的任一维——也就是隐层的一个 cell——都认为会受到源空间的每一维的影响。不考虑严谨，可以说，目标向量是源向量的加权和。在 CNN 中，全连接常出现在最后几层，用于对前面设计的特征做加权和。比如 mnist，前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。（卷积相当于全连接的有意弱化，按照局部视野的启发，把局部之外的弱影响直接抹为零影响；还做了一点强制，不同的局部所使用的参数居然一致。弱化使参数变少，节省计算量，又专攻局部不贪多求全；强制进一步减少参数。少即是多）

**总结：FC相当于浓缩特征，是一种强化的卷积。**

### 2.3 与U-net的联系

U-net属于FCN框架，FCN是输入和输出都是图像，没有全连接层。较浅的高分辨率层用来解决像素定位的问题，较深的层用来解决像素分类的问题。属于端到端的学习，图像风格转换以及图像超分辨率都是这类框架。

## 3.应用背景补充

### 3.1图像分类与图像分割

图像分类就是输入一张图片输出一个标签。图像分割是对每一个像素进行分类，判断是属于背景还是前景。分类一般采用CNN，分割一般采用FCN。

### 3.2图像分割FCN的一些知识

分为下卷积层和上卷积层，下卷积层用于提取特征，上卷积层（反卷积）用于恢复图像。在提取完特征之后要进行一个特征融合，在融合的过程中图像中的分割情况会越来越明显。（如下图所示，pool5与pool4融合后更好，pool3-5融合后效果最好）

注：下卷积和上卷积，又称**编码解码**

![image-20210131111741348](C:\Users\14523\AppData\Roaming\Typora\typora-user-images\image-20210131111741348.png)

## 4. U-net概述

（Next day）